---
---

@article{Appel2023a,
abstract = {Social media companies have come under increasing pressure to remove misinformation from their platforms, but partisan disagreements over what should be removed have stymied efforts to deal with misinformation in the United States. Current explanations for these disagreements center on the “fact gap”—differences in perceptions about what is misinformation. We argue that partisan differences could also be due to “party promotion”—a desire to leave misinformation online that promotes one's own party—or a “preference gap”—differences in internalized preferences about whether misinformation should be removed. Through an experiment where respondents are shown false headlines aligned with their own or the opposing party, we find some evidence of party promotion among Democrats and strong evidence of a preference gap between Democrats and Republicans. Even when Republicans agree that content is false, they are half as likely as Democrats to say that the content should be removed and more than twice as likely to consider removal as censorship.},
author = {Appel, Ruth E. and Pan, Jennifer and Roberts, Margaret E.},
doi = {10.1126/sciadv.adg6799},
html = {https://doi.org/10.1126/sciadv.adg6799},
journal = {Science Advances},
number = {44},
pages = {1--10},
title = {{Partisan conflict over content moderation is more than disagreement about facts}},
volume = {9},
year = {2023},
selected = {true}
}

@incollection{Matz2022,
author = {Matz, Sandra C. and Appel, Ruth E. and Croll, Brian},
booktitle = {The psychology of technology: Social science research in the age of Big Data},
doi = {10.1037/0000290-012},
html = {https://doi.org/10.1037/0000290-012},
editor = {Matz, Sandra C.},
pages = {379--420},
publisher = {American Psychological Association},
title = {{Privacy and ethics in the age of Big Data}},
abstract = {This chapter discusses the new ethical challenges introduced by the age of Big Data. Although there are many other ethical challenges related to technology and data (e.g., addiction, inequality), it reviews the topic of privacy as one of the major challenges associated with Big Data. The chapter introduces the concept of privacy, briefly discussing its history, universality, and core assumptions that lie at the heart of privacy protections. It then moves on to the questions of how Big Data threatens our privacy in unprecedented ways and challenges current approaches to privacy protection. Next, the chapter discusses how placing the burden of privacy protection on users alone is misguided and provide a number of potential systemic solutions related to regulation, collaboration, design principles, and technological tools. It concludes with concrete practical guidelines for researchers and practitioners of how to design studies, products, and services that protect individuals' privacy.},
year = {2022}
}

@incollection{Appel2021,
author = {Appel, Ruth E. and Matz, Sandra C.},
booktitle = {Measuring and Modeling Persons and Situations},
doi = {10.1016/b978-0-12-819200-9.00015-6},
html = {https://doi.org/10.1016/b978-0-12-819200-9.00015-6},
isbn = {9780128192009},
abstract = {Advances in the collection, storage, and processing of large amounts of user data have given rise to psychological targeting, which we define as the process of extracting individuals’ psychological characteristics from their digital footprints in order to target them with psychologically-informed interventions at scale. In this chapter, we introduce a two-stage framework of psychological targeting consisting of (1) psychological profiling and (2) psychologically-informed interventions. We summarize the most important research findings in relation to the two stages and discuss important methodological opportunities and pitfalls. To help researchers make the most of the opportunities, we also provide practical advice on how to deal with some of the potential pitfalls. Finally, we highlight ethical opportunities and challenges and offer some suggestions for addressing these challenges. If done right, psychological targeting has the potential to advance our scientific understanding of human nature and to enhance the well-being of individuals and society at large.},
keywords = {Big Data,Contextual integrity,Digital footprints,Ethics,Methods,Privacy,Psychological profiling,Psychological targeting,Psychologically-informed interventions},
pages = {193--222},
publisher = {Elsevier},
title = {{Psychological targeting in the age of Big Data}},
url = {http://dx.doi.org/10.1016/B978-0-12-819200-9.00015-6},
year = {2021}
}

@article{Matz2020,
abstract = {Psychological targeting describes the practice of extracting people's psychological profiles from their digital footprints (e.g., their Facebook Likes, Tweets or credit card records) in order to influence their attitudes, emotions or behaviors through psychologically-informed interventions at scale. We discuss how the increasingly blurred lines between public and private information, and the continuation of the outdated practices of notice and consent, challenge traditional conceptualizations of privacy in the context of psychological targeting. Drawing on the theory of contextual integrity, we argue that it is time to rethink privacy and move beyond the questions of who collects what data to how the data are being used. Finally, we suggest that regulations of psychological targeting should be accompanied by a mindset that fosters (1) privacy by design to make it easy for individuals to act in line with their privacy goals, as well as (2) disclosure by choice in which individuals can freely decide whether and when they might be willing to forsake their privacy for better service.},
author = {Matz, Sandra C. and Appel, Ruth E. and Kosinski, Michal},
doi = {10.1016/j.copsyc.2019.08.010},
html = {https://doi.org/10.1016/j.copsyc.2019.08.010},
issn = {2352250X},
journal = {Current Opinion in Psychology},
pages = {116--121},
title = {{Privacy in the age of psychological targeting}},
volume = {31},
year = {2020}
}

@workshop{Appelworkshop1,
author = {Ruth E. Appel},
title = {{Generative AI Regulation Can Learn from Social Media Regulation}},
abstract = {  There is strong agreement that generative AI should be regulated, but strong disagreement on how to approach regulation. While some argue that AI regulation should mostly rely on extensions of existing laws, others argue that entirely new laws and regulations are needed to ensure that generative AI benefits society. In this paper, I argue that the debates on generative AI regulation can be informed by the debates and evidence on social media regulation. For example, AI companies have faced allegations of political bias regarding the images and text their models produce, similar to the allegations social media companies have faced regarding content ranking on their platforms. First, I compare and contrast the affordances of generative AI and social media to highlight their similarities and differences. Then, I discuss specific policy recommendations based on the evolution of social media and their regulation. These recommendations include investments in: efforts to counter bias and perceptions thereof (e.g., via transparency, researcher access, oversight boards, democratic input, research studies), specific areas of regulatory concern (e.g., youth wellbeing, election integrity) and trust and safety, computational social science research, and a more global perspective. Applying lessons learnt from social media regulation to generative AI regulation can save effort and time, and prevent avoidable mistakes.},
url = {https://doi.org/10.48550/arXiv.2412.11335},
html = {https://doi.org/10.48550/arXiv.2412.11335},
year = {2024}
}

@under_review{Appelunderreview6,
author = {Ruth E. Appel and Young Mie Kim and Jennifer Pan and Yiqing Xu and Daniel R. Thomas and Hunt Allcott and Pablo Barberá and Taylor Brown and Adriana Crespo-Tenorio and Drew Dimmery and Deen Freelon and Matthew Gentzkow and Andrew M. Guess and Sandra González-Bailón and Shanto Iyengar and David Lazer and Neil Malhotra and Devra Moehler and Ben Nimmo and Brendan Nyhan and Jaime Settle and Emily Thorson and Rebekah Tromble and Carlos Velasco Rivera and Arjun Wilkins and Magdalena Wojcieszak and Beixian Xiong and Chad Kiewiet de Jonge and Annie Franco and Winter Mason and Natalie Jomini Stroud and Joshua A. Tucker},
title = {{How deceptive online networks reached millions in the US 2020 elections}},
year = {2025}
}

@under_review{Appelunderreview5,
author = {Appel, Ruth E. and Roozenbeek, Jon and Rayburn-Reeves, Rebecca and Corbin, Jonathan and Basol, Melisa and Compton, Joshua and {van der Linden}, Sander},
title = {{Psychological inoculation improves resilience to and reduces willingness to share vaccine misinformation}},
abstract = {Vaccine misinformation endangers public health by contributing to reduced vaccine uptake. We developed a short online game to reduce people's susceptibility to vaccine misinformation. Building on inoculation theory, the Bad Vaxx game exposes people to weakened doses of manipulation techniques commonly used in vaccine misinformation and to strategies to identify these techniques. Across three preregistered randomized controlled trials (N=2,326), we find that the game significantly improves participants' ability to discern vaccine misinformation from non-misinformation, their confidence in their ability to do so, and the quality of their sharing decisions. Further, taking the perspective of a character fighting as opposed to spreading misinformation is more effective. In line with the learning goals of the intervention, we show that participants improve their ability to correctly identify the use of specific misinformation techniques. This insight is important because teaching manipulation technique recognition is not only effective to help evaluate information about vaccines, but also more viable than trying to teach myriads of constantly-evolving facts. Our findings suggest that a short, low-cost, gamified intervention can increase resilience to vaccine misinformation.},
url = {https://osf.io/preprints/psyarxiv/ek5pu},
html = {https://osf.io/preprints/psyarxiv/ek5pu},
year = {2025}
}

@under_review{Appelunderreview4,
author = {Pei, Rui and Grayson, Samantha J. and Appel, Ruth E. and Soh, Serena and Garcia, Sydney and Huang, Emily and Jackson, Matthew O. and Harari, Gabriella M. and Zaki, Jamil},
title = {{Bridging the empathy gap: Reducing empathy misperceptions increases social connectedness}},
year = {2025}
}

@inproceedings{Appel2025a,
author = {Ruth E. Appel},
title = {{Generative AI Regulation Can Learn from Social Media Regulation}},
abstract = {There is strong agreement that generative AI should be regulated, but strong disagreement on how to approach regulation. While some argue that AI regulation should mostly rely on extensions of existing laws, others argue that entirely new laws and regulations are needed to ensure that generative AI benefits society. In this paper, I argue that the debates on generative AI regulation can be informed by the debates and evidence on social media regulation. For example, AI companies have faced allegations of political bias regarding the images and text their models produce, similar to the allegations social media companies have faced regarding content ranking on their platforms. First, I compare and contrast the affordances of generative AI and social media to highlight their similarities and differences. Then, I discuss specific policy recommendations based on the evolution of social media and their regulation. These recommendations include investments in: efforts to counter bias and perceptions thereof (e.g., via transparency, researcher access, oversight boards, democratic input, research studies), specific areas of regulatory concern (e.g., youth wellbeing, election integrity) and trust and safety, computational social science research, and a more global perspective. Applying lessons learnt from social media regulation to generative AI regulation can save effort and time, and prevent avoidable mistakes.},
url = {https://doi.org/10.48550/arXiv.2412.11335},
html = {https://doi.org/10.48550/arXiv.2412.11335},
year = {2025},
month = {July},
booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
note = {Position Track (oral)}
}

@inproceedings{Appel2025b,
author = {Jillian Fisher and Ruth E. Appel and Chan Young Park and Yujin Potter and Liwei Jiang and Taylor Sorensen and Shangbin Feng and Yulia Tsvetkov and Margaret Roberts and Jennifer Pan and Dawn Song and Yejin Choi },
abstract = {AI systems often exhibit political bias, influencing users' opinions and decision-making. While political neutrality—defined as the absence of bias—is often seen as an ideal solution for fairness and safety, this position paper argues that true political neutrality is neither feasible nor universally desirable due to its subjective nature and the biases inherent in AI training data, algorithms, and user interactions. However, inspired by Joseph Raz's philosophical insight that "neutrality [...] can be a matter of degree" (Raz, 1986), we argue that striving for some neutrality remains essential for promoting balanced AI interactions and mitigating user manipulation. Therefore, we use the term "approximation" of political neutrality to shift the focus from unattainable absolutes to achievable, practical proxies. We propose eight techniques for approximating neutrality across three levels of conceptualizing AI, examining their trade-offs and implementation strategies. In addition, we explore two concrete applications of these approximations to illustrate their practicality. Finally, we assess our framework on current large language models (LLMs) at the output level, providing a demonstration of how it can be evaluated. This work seeks to advance nuanced discussions of political neutrality in AI and promote the development of responsible, aligned language models.},
title = {{Political Neutrality in AI is Impossible — But Here Is How to Approximate It}},
url = {https://doi.org/10.48550/arXiv.2503.05728},
html = {https://doi.org/10.48550/arXiv.2503.05728},
year = {2025},
month = {July},
booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
note = {Position Track (oral)}
}

@inproceedings{Appel2025c,
author = {Shayne Longpre and Kevin Klyman and Ruth E. Appel and Sayash Kapoor and Rishi Bommasani and Michelle Sahar and Sean McGregor and Avijit Ghosh and Borhane Blili-Hamelin and Nathan Butters and Alondra Nelson and Dr. Amit Elazari and Andrew Sellars and Casey John Ellis and Dane Sherrets and Dawn Song and Harley Geiger and Ilona Cohen and Lauren McIlvenny and Madhulika Srikumar and Mark M. Jaycox and Markus Anderljung and Nadine Farid Johnson and Nicholas Carlini and Nicolas Miailhe and Nik Marda and Peter Henderson and Rebecca S. Portnoff and Rebecca Weiss and Victoria Westerhoff and Yacine Jernite and Rumman Chowdhury and Percy Liang and Arvind Narayanan},
title = {{In-House Evaluation Is Not Enough. Towards Robust Third-Party Evaluation and Flaw Disclosure for General-Purpose AI}},
abstract = {The widespread deployment of general-purpose AI (GPAI) systems introduces significant new risks. 
Yet the infrastructure, practices, and norms for reporting flaws in GPAI systems remain seriously underdeveloped, lagging far behind more established fields like software security. Based on a collaboration between experts from the fields of software security, machine learning, law, social science, and policy, we identify key gaps in the evaluation and reporting of flaws in GPAI systems. We call for three interventions to advance system safety.
First, we propose using standardized AI flaw reports and rules of engagement for researchers in order to ease the process of submitting, reproducing, and triaging flaws in GPAI systems. Second, we propose GPAI system providers adopt broadly-scoped flaw disclosure programs, borrowing from bug bounties, with legal safe harbors to protect researchers. Third, we advocate for the development of improved infrastructure to coordinate distribution of flaw reports across the many stakeholders who may be impacted. These interventions are increasingly urgent, as evidenced by the prevalence of jailbreaks and other flaws that can transfer across different providers' GPAI systems. By promoting robust reporting and coordination in the AI ecosystem, these proposals could significantly improve the safety, security, and accountability of GPAI systems.},
url = {https://crfm.stanford.edu/2025/03/13/thirdparty.html},
html = {https://crfm.stanford.edu/2025/03/13/thirdparty.html},
year = {2025},
month = {July},
booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
note = {Position Track (spotlight)}
}

@work_in_progress{Appelunpublished3,
author = {Ruth E. Appel and Jennifer Pan and Margaret E. Roberts},
title = {{How partisanship affects preferences for content moderation in large language models}},
year = {2025}
}

@work_in_progress{Appelunpublished1,
author = {Appel, Ruth E. and Athey, Susan and Karlan, Dean and Koutout, Kristine and Luca, Michael and Manjeer, Utsav and Sacher, Szymon and Wernerfelt, Nils},
title = {{Combating misinformation on social media}},
year = {2025}
}

@policy{Bommasani2025,
author = {Bommasani, R. and Singer, S. and Appel, R. E. and Cen, S. and Cooper, A. F. and Cryst, E. and Gailmard, L. A. and Gonzalez, J. E. and Ho, D. E. and Klaus, I. and Lee, M. M. and Liang, P. and Reuel, A. and Song, D. and Spence, D. and Wan and A., Wang, A. and Zhang, D. and Zittrain, J. and Chayes, J. T. and Cuéllar, M.-F. and Fei-Fei, L.},
month = {mar},
title = {{Report of the Joint California Policy Working Group on AI Frontier Models}},
url = {https://www.cafrontieraigov.org/},
html = {https://www.cafrontieraigov.org/},
year = {2025}
}

@policy{Appel2024,
author = {Appel, Ruth E.},
month = {mar},
title = {{EU DSA Election Guidelines Input}},
url = {https://ruthappel.github.io/assets/pdf/EU-DSA-Election-Guidelines-Input.pdf},
html = {https://ruthappel.github.io/assets/pdf/EU-DSA-Election-Guidelines-Input.pdf},
year = {2024}
}

@policy{Schaake2020,
author = {Schaake, Marietje and Appel, Ruth E. and Duplichen, Dathan M. and Einstein, Lisa and Elhai, Wren and {Dhafer Muhammad Faishal}, Muhammad and Foryciarz, Agata and Frankenberg, Sydney L. and Friedman, Toni and Huczok, Zoe and Jasper, Kyra and Jablanski, Danielle and King, Jennifer and Kuang, Cindy and Lee, Heajune and Mantha, Shreya and Patil, Vidyangi and Portelance, Gailyn and Stepahn, Adriana and Tamkin, Alex and Vecchiato, Alessandro and Zhang, Eva and Zhao, Jason},
month = {jun},
pages = {1--23},
title = {{Input on the European Commission White Paper "On Artificial Intelligence – A European approach to excellence and trust"}},
url = {https://hai.stanford.edu/sites/default/files/2020-07/HAI{\_}WhitePaper{\_}v4B.pdf},
html = {https://hai.stanford.edu/sites/default/files/2020-07/HAI{\_}WhitePaper{\_}v4B.pdf},
year = {2020}
}

@media{Longpre2025,
author = {Longpre, S. and Appel, Ruth E.},
month = {mar},
title = {{General-Purpose AI Needs Coordinated Flaw Reporting}},
url = {https://crfm.stanford.edu/2025/03/13/thirdparty.html},
html = {https://crfm.stanford.edu/2025/03/13/thirdparty.html},
year = {2025}
}

@media{Appel2024c,
author = {Appel, Ruth E.},
journal = {Freedom to Tinker},
month = {nov},
title = {{Strengthening AI Accountability Through Better Third Party Evaluations}},
url = {https://freedom-to-tinker.com/2024/11/25/strengthening-ai-accountability-through-better-third-party-evaluations-part-1/ },
html = {https://freedom-to-tinker.com/2024/11/25/strengthening-ai-accountability-through-better-third-party-evaluations-part-1/ },
year = {2024}
}

@media{Appel2024b,
author = {Appel, Ruth E.},
journal = {HAI},
month = {nov},
title = {{Strengthening AI Accountability Through Better Third Party Evaluations}},
url = {https://hai.stanford.edu/news/strengthening-ai-accountability-through-better-third-party-evaluations},
html = {https://hai.stanford.edu/news/strengthening-ai-accountability-through-better-third-party-evaluations},
year = {2024}
}

@media{Appel2023b,
author = {Appel, Ruth E.},
journal = {The Conversation},
month = {nov},
title = {{It's not just about facts: Democrats and Republicans have sharply different attitudes about removing misinformation from social media}},
url = {https://theconversation.com/its-not-just-about-facts-democrats-and-republicans-have-sharply-different-attitudes-about-removing-misinformation-from-social-media-216809},
html = {https://theconversation.com/its-not-just-about-facts-democrats-and-republicans-have-sharply-different-attitudes-about-removing-misinformation-from-social-media-216809},
year = {2023}
}

@media{Matz2019,
author = {Matz, Sandra C. and Appel, Ruth E. and Kosinski, Michal},
booktitle = {LSE Business Review},
title = {{Rethinking privacy in the age of psychological targeting}},
url = {https://blogs.lse.ac.uk/businessreview/2019/11/21/rethinking-privacy-in-the-age-of-psychological-targeting/},
html = {https://blogs.lse.ac.uk/businessreview/2019/11/21/rethinking-privacy-in-the-age-of-psychological-targeting/},
urldate = {2020-01-06},
year = {2019}
}