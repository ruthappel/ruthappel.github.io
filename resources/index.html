<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Resources | Ruth E. Appel</title> <meta name="author" content="Ruth E. Appel"> <meta name="description" content=""> <meta name="keywords" content="ruth appel, computational social science, content moderation, large language models, behavioral economics"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/site_pic.png?9fa3c57547d262bec3c42aabdc979a1e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ruthappel.github.io/resources/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Ruth E. Appel</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/workinprogress/">Work in Progress</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae</a> </li> <li class="nav-item active"> <a class="nav-link" href="/resources/">Resources<span class="sr-only">(current)</span></a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Resources</h1> <p class="post-description"></p> </header> <article> <article> <div class="clearfix"> <h2 id="tech-policy-resources">Tech Policy Resources</h2> <p>Last updated: October 16, 2024</p> <ul> <li> <a href="#newsletters">Newsletters</a> <ul> <li><a href="#tech-policy-in-general">Tech policy in general</a></li> <li><a href="#ai-focus">AI focus</a></li> </ul> </li> <li><a href="#podcasts">Podcasts</a></li> <li><a href="#research-centers">Research Center</a></li> <li> <a href="#conferences">Conferences</a> <ul> <li><a href="#ai-related-conferences">AI-related conferences</a></li> <li><a href="#ai-ethics">AI ethics</a></li> <li><a href="#trust-and-safety">Trust and safety</a></li> </ul> </li> <li><a href="#events">Events</a></li> <li> <a href="#selected-ai-reading-recommendations">Selected AI reading recommendations</a> <ul> <li><a href="#understanding-sources-of-bias">Understanding sources of bias</a></li> <li><a href="#thinking-about-risk">Thinking about risk</a></li> <li><a href="#approaches-to-alignment">Approaches to alignment</a></li> <li><a href="#evaluation-challenges">Evaluation challenges</a></li> <li><a href="#benchmarks-and-evaluations">Benchmarks and evaluations</a></li> <li><a href="#persuasion">Persuasion</a></li> <li><a href="#deceptive-campaigns-and-misinformation">Deceptive campaigns and misinformation</a></li> <li><a href="#language-and-its-impact">Language and its impact</a></li> <li><a href="#ai-incident-trackers">AI incident trackers</a></li> <li><a href="#model-transparency">Model transparency</a></li> <li><a href="#positive-use-cases">Positive use cases</a></li> <li><a href="#perceptions-regarding-generative-ai">Perceptions of generative AI</a></li> <li><a href="#ai-ethics-classics">AI ethics classics</a></li> <li><a href="#ai-policy-overviews">AI policy overviews</a></li> </ul> </li> <li><a href="#ai-safety-related-organizations">AI-safety related organizations</a></li> <li><a href="#stanford-groups">Stanford groups</a></li> <li><a href="#jobinternship-funding-opportunities">Job/internship opportunities</a></li> </ul> <h3 id="newsletters">Newsletters</h3> <h4 id="tech-policy-in-general">Tech policy in general</h4> <ul> <li><a href="https://gmail.us8.list-manage.com/subscribe?u=42fbf413961fc8bc2584ac25f&amp;id=3beb31fac3&amp;utm_source=Schaake+Newsletter&amp;utm_campaign=7097704efd-EMAIL_CAMPAIGN_2022_04_12_06_47_COPY_01&amp;utm_medium=email&amp;utm_term=0_3beb31fac3-7097704efd-538327157" rel="external nofollow noopener" target="_blank">Tech Policy Watch newsletter</a></li> <li><a href="https://cyber.fsi.stanford.edu/content/newsletters-cpc" rel="external nofollow noopener" target="_blank">Stanford Cyber Policy Center newsletter</a></li> <li><a href="http://harvard.us10.list-manage2.com/subscribe?u=ef13e6d75b74b1791f13115cd&amp;id=4782a3c945" rel="external nofollow noopener" target="_blank">Harvard Berkman Klein Center Newsletter</a></li> </ul> <h4 id="ai-focus">AI focus</h4> <ul> <li><a href="https://importai.substack.com/" rel="external nofollow noopener" target="_blank">Import AI</a></li> <li><a href="https://newsletter.safe.ai/subscribe" rel="external nofollow noopener" target="_blank">AI Safety Newsletter</a></li> <li><a href="https://cset.georgetown.edu/sign-up/" rel="external nofollow noopener" target="_blank">Georgetown CSET newsletter</a></li> <li><a href="https://chinai.substack.com/" rel="external nofollow noopener" target="_blank">ChinAI</a></li> </ul> <h3 id="podcasts">Podcasts</h3> <ul> <li><a href="https://podcasts.apple.com/us/podcast/hard-fork/id1528594034" rel="external nofollow noopener" target="_blank">Hard Fork</a></li> <li><a href="https://www.csis.org/podcasts/ai-policy-podcast" rel="external nofollow noopener" target="_blank">The AI Policy Podcast</a></li> <li><a href="https://www.politico.com/podcasts/tech" rel="external nofollow noopener" target="_blank">POLITICO Tech</a></li> <li><a href="https://podcasts.apple.com/us/podcast/arbiters-of-truth/id1607949880" rel="external nofollow noopener" target="_blank">Arbiters of Truth</a></li> <li><a href="https://podcasts.apple.com/us/podcast/moderated-content/id1646510125" rel="external nofollow noopener" target="_blank">Moderated Content</a></li> <li><a href="https://deepmind.google/discover/the-podcast/" rel="external nofollow noopener" target="_blank">Google DeepMind: The Podcast</a></li> <li><a href="https://www.microsoft.com/en-us/research/podcast/" rel="external nofollow noopener" target="_blank">Microsoft Research Podcast</a></li> </ul> <h3 id="research-centers">Research Centers</h3> <ul> <li> <a href="https://hai.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford HAI</a> <ul> <li><a href="https://hai.stanford.edu/centers" rel="external nofollow noopener" target="_blank">Center for Research on Foundation Models (CFRM)</a></li> <li><a href="https://reglab.stanford.edu/" rel="external nofollow noopener" target="_blank">RegLab</a></li> </ul> </li> <li><a href="https://ethicsinsociety.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford McCoy Family Center for Ethics in Society</a></li> <li> <a href="https://cyber.fsi.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford Cyber Policy Center</a> (has multiple relevant centers like the <a href="https://cyber.fsi.stanford.edu/io" rel="external nofollow noopener" target="_blank">Stanford Internet Observatory</a> or <a href="https://cyber.fsi.stanford.edu/content/program-governance-emerging-technologies" rel="external nofollow noopener" target="_blank">Governance of Emerging Technologies</a> and hosts many events and a lunch speaker series)</li> <li><a href="https://cset.georgetown.edu/" rel="external nofollow noopener" target="_blank">Georgetown Center for Security and Emerging Technology (CSET)</a></li> </ul> <h3 id="conferences">Conferences</h3> <h4 id="ai-related-conferences">AI-related conferences</h4> <ul> <li><a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">International Conference on Learning Representations (ICLR)</a></li> <li><a href="https://2025.emnlp.org/" rel="external nofollow noopener" target="_blank">Conference on Empirical Methods in Natural Language Processing (EMNLP)</a></li> <li><a href="https://kdd2025.kdd.org/" rel="external nofollow noopener" target="_blank">Knowledge Discovery and Data Mining (KDD)</a></li> <li><a href="https://www2025.thewebconf.org/" rel="external nofollow noopener" target="_blank">The Web Conference (The Web Conference (WWW))</a></li> <li><a href="https://cvpr.thecvf.com/" rel="external nofollow noopener" target="_blank">IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</a></li> <li><a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">Conference and Workshop on Neural Information Processing Systems (NeurIPS)</a></li> <li><a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">International Conference on Machine Learning (ICML)</a></li> <li><a href="https://www.sigsac.org/ccs/CCS2025" rel="external nofollow noopener" target="_blank">Conference on Computer and Communications Security (CCS)</a></li> <li><a href="https://sp2025.ieee-security.org/index.html" rel="external nofollow noopener" target="_blank">IEEE Symposium on Security and Privacy (IEEE S&amp;P)</a></li> <li><a href="https://cscw.acm.org/" rel="external nofollow noopener" target="_blank">Conference on Computer-Supported Cooperative Work and Social Computing (CSCW)</a></li> <li><a href="https://compass.acm.org/" rel="external nofollow noopener" target="_blank">Conference on Computing and Sustainable Societies (COMPASS)</a></li> <li><a href="https://2025.aclweb.org/" rel="external nofollow noopener" target="_blank">Annual Meeting of the Association for Computational Linguistics (ACL)</a></li> <li><a href="https://chi2025.acm.org/" rel="external nofollow noopener" target="_blank">CHI conference on Human Factors in Computing Systems (CHI)</a></li> <li><a href="https://ic2s2.org/" rel="external nofollow noopener" target="_blank">International Conference for Computational Social Science (IC2S2)</a></li> <li><a href="https://dis.acm.org/2025/" rel="external nofollow noopener" target="_blank">Conference on Designing Interactive Systems (DIS)</a></li> </ul> <h4 id="ai-ethics">AI ethics</h4> <ul> <li><a href="https://facctconference.org/" rel="external nofollow noopener" target="_blank">ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)</a></li> <li><a href="https://www.eaamo.org/" rel="external nofollow noopener" target="_blank">EAAMO (Equity and Access in Algorithms, Mechanisms, and Optimization)</a></li> </ul> <h4 id="trust-and-safety">Trust and safety</h4> <ul> <li><a href="https://conferences.law.stanford.edu/tsrc/" rel="external nofollow noopener" target="_blank">Trust and Safety Research Conference</a></li> <li><a href="https://www.trustcon.net/" rel="external nofollow noopener" target="_blank">TrustCon</a></li> </ul> <h3 id="events">Events</h3> <ul> <li> <a href="https://docs.google.com/spreadsheets/d/1P6ut7vL-gXKbeDeh3nuPqBjoCupjIt87Sw7TnhumBSU/edit#gid=1781893986" rel="external nofollow noopener" target="_blank">GenAI Events in the Bay Area</a> (this links to an amazing, almost overwhelming spreadsheet)</li> </ul> <h3 id="selected-ai-reading-recommendations">Selected AI reading recommendations</h3> <h4 id="understanding-sources-of-bias">Understanding sources of bias</h4> <ul> <li>Suresh, H., &amp; Guttag, J. V. (2021). <a href="https://doi.org/3465416.3483305" rel="external nofollow noopener" target="_blank">A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle</a>. In <em>EAAMO 2021: Equity and Access in Algorithms, Mechanisms, and Optimization</em>.</li> <li>Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., … Gabriel, I. (2021). <a href="http://arxiv.org/abs/2112.04359" rel="external nofollow noopener" target="_blank">Ethical and social risks of harm from Language Models</a>.</li> <li>Ferrara, E. (2023). <a href="http://arxiv.org/abs/2304.03738" rel="external nofollow noopener" target="_blank">Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models</a>.</li> </ul> <h4 id="thinking-about-risk">Thinking about risk</h4> <ul> <li>Kapoor, S., Bommasani, R., Klyman, K., Longpre, S., Ramaswami, A., Cihon, P., … Engler, A. (2024). <a href="https://crfm.stanford.edu/open-fms/paper.pdf" rel="external nofollow noopener" target="_blank">On the Societal Impact of Open Foundation Models</a>. <ul> <li>Presents helpful framework to analyze risks, and highlights the need to account for context and assess marginal risk</li> <li> <strong>Summary:</strong> <ul> <li>Risks of LLMs should be evaluated in terms of their marginal risks over existing technologies</li> <li>Framework to analyze risks: 1) identify threat 2) evaluate existing risks (without open foundation models) 3) evaluating existing defenses (without open foundation models) 4) assess evidence of marginal risks through open foundation models 5) assess potential for open foundation models in assisting defense against risks 6) describe assumptions and uncertainties of assessment</li> <li>Open foundation models have several benefits, including giving users the power to shape acceptable model behavior, increasing innovation, accelerating science, enabling transparency and mitigating market concentration.</li> </ul> </li> </ul> </li> <li>Narayanan, A., &amp; Kapoor, S. (2024). <a href="https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property" rel="external nofollow noopener" target="_blank">AI safety is not a model property</a>. <ul> <li>Highlights importance of considering context</li> </ul> </li> <li>Mökander, J., Schuett, J., Kirk, H. R., &amp; Floridi, L. (2023). <a href="https://doi.org/10.1007/s43681-023-00289-2" rel="external nofollow noopener" target="_blank">Auditing Large Language Models: A Three-Layered Approach</a>. <em>AI and Ethics</em>. Springer International Publishing.</li> <li>Yang, E., &amp; Roberts, M. E. (2023). <a href="https://doi.org/10.1353/jod.2023.a907695" rel="external nofollow noopener" target="_blank">The Authoritarian Data Problem</a>. <em>Journal of Democracy</em>, <em>34</em>(4), 141–150.</li> </ul> <h4 id="approaches-to-alignment">Approaches to alignment</h4> <ul> <li>Anthropic. (2023). <a href="https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input" rel="external nofollow noopener" target="_blank">Collective Constitutional AI: Aligning a Language Model with Public Input</a>.</li> <li>Perrigo, B. (2024, February). <a href="https://time.com/6684266/openai-democracy-artificial-intelligence/" rel="external nofollow noopener" target="_blank">Inside OpenAI’s Plan to Make AI More ‘Democratic.’</a> <em>TIME</em>.</li> <li>Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., … Kaplan, J. (2022). <a href="http://arxiv.org/abs/2212.08073" rel="external nofollow noopener" target="_blank">Constitutional AI: Harmlessness from AI Feedback</a>.</li> <li>Ganguli, D., Askell, A., Schiefer, N., Liao, T. I., Lukošiūtė, K., Chen, A., … Kaplan, J. (2023). <a href="http://arxiv.org/abs/2302.07459" rel="external nofollow noopener" target="_blank">The Capacity for Moral Self-Correction in Large Language Models</a>.</li> <li>Robinson, D. G. (2022). <a href="https://slate.com/technology/2022/08/kidney-allocation-algorithm-ai-ethics.html" rel="external nofollow noopener" target="_blank">The Kidney Transplant Algorithm’s Surprising Lessons for Ethical A.I.</a> <ul> <li> <strong>Summary:</strong> <ul> <li>Kidney organ donation algorithm as an example of how AI can be governed more democratically Recognizing the moral decisions behind technical ones</li> <li>Broad public input, not only expert decision-making</li> <li>Transparent decision-making</li> <li>External auditing</li> <li>Ability to forecast what changes in the system would mean</li> </ul> </li> </ul> </li> </ul> <h4 id="evaluation-challenges">Evaluation challenges</h4> <ul> <li>Anthropic. (2023). <a href="https://www.anthropic.com/news/evaluating-ai-systems" rel="external nofollow noopener" target="_blank">Challenges in evaluating AI systems</a>.</li> <li>Friedler, S., Singh, R., Blili-Hamelin, B., Metcalf, J., &amp; Chen, B. J. (2023). <a href="https://datasociety.net/wp-content/uploads/2023/10/Recommendations-for-Using-Red-Teaming-for-AI-Accountability-PolicyBrief.pdf" rel="external nofollow noopener" target="_blank">AI Red-Teaming Is Not a One-Stop Solution to AI Harms: Recommendations for Using Red-Teaming for AI Accountability</a>.</li> </ul> <h4 id="benchmarks-and-evaluations">Benchmarks and evaluations</h4> <ul> <li>Bommasani, R., Liang, P., &amp; Lee, T. (2022). <a href="https://arxiv.org/pdf/2211.09110.pdf" rel="external nofollow noopener" target="_blank">Holistic Evaluation of Language Models</a>.</li> <li>Durmus, E., Nyugen, K., Liao, T. I., Schiefer, N., Askell, A., Bakhtin, A., … Ganguli, D. (2023). <a href="http://arxiv.org/abs/2306.16388" rel="external nofollow noopener" target="_blank">Towards Measuring the Representation of Subjective Global Opinions in Language Models</a>.</li> <li>Röttger, P., Hofmann, V., Pyatkin, V., Hinck, M., Kirk, H. R., Schütze, H., &amp; Hovy, D. (2024). <a href="http://arxiv.org/abs/2402.16786" rel="external nofollow noopener" target="_blank">Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models</a>. <ul> <li>Interesting critique of standard survey-based model evaluations, evidence of political bias</li> </ul> </li> <li>Hofmann, V., Kalluri, P. R., Jurafsky, D., &amp; King, S. (2024). <a href="https://doi.org/10.1038/s41586-024-07856-5" rel="external nofollow noopener" target="_blank">AI generates covertly racist decisions about people based on their dialect</a>. <em>Nature</em>, 633(February).</li> </ul> <h4 id="persuasion">Persuasion</h4> <ul> <li>Anthropic. (2024). <a href="https://www.anthropic.com/news/measuring-model-persuasiveness" rel="external nofollow noopener" target="_blank">Measuring the Persuasiveness of Language Models</a>.</li> <li>Bai, H., Voelkel, J. G., Eichstaedt, J. C., &amp; Willer, R. (2023). <a href="https://doi.org/10.31219/osf.io/stakv" rel="external nofollow noopener" target="_blank">Artificial Intelligence Can Persuade Humans on Political Issues</a>.</li> <li>Matz, S. C., Teeny, J. D., Vaid, S. S., Harari, G. M., &amp; Cerf, M. (2024). <a href="https://doi.org/10.1038/s41598-024-53755-0" rel="external nofollow noopener" target="_blank">The potential of generative AI for personalized persuasion at scale</a>. <em>Nature Scientific Reports</em>, <em>14</em>(4692), 1–16.</li> <li>Kobi Hackenburg, &amp; Margetts, H. (2024). <a href="https://doi.org/10.1073/pnas.2403116121" rel="external nofollow noopener" target="_blank">Evaluating the persuasive influence of political microtargeting with large language models</a>. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>.</li> <li>Salvi, F., Ribeiro, M. H., Gallotti, R., West, R., &amp; Mar, C. Y. (2024). <a href="https://arxiv.org/pdf/2403.14380" rel="external nofollow noopener" target="_blank">On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial</a>.</li> <li>Jakesch, M., Bhat, A., Buschek, D., Zalmanson, L., &amp; Naaman, M. (2023). <a href="https://doi.org/10.1145/3544548.35811960" rel="external nofollow noopener" target="_blank">Co-Writing with Opinionated Language Models Affects Users’ Views</a>. In Conference on Human Factors in Computing Systems.</li> <li>Williams-Ceci, S., Jakesch, M., Bhat, A., Kadoma, K., Zalmanson, L., &amp; Naaman, M. (2024). <a href="https://doi.org/10.31234/osf.io/mhjn6" rel="external nofollow noopener" target="_blank">Bias in AI Autocomplete Suggestions Leads to Attitude Shift on Societal Issues</a>.</li> <li>Fisher, J., Feng, S., Aron, R., Richardson, T., Choi, Y., Fisher, D. W., … Reinecke, K. (2024). <a href="http://arxiv.org/abs/2410.06415" rel="external nofollow noopener" target="_blank">Biased AI can Influence Political Decision-Making</a>.</li> </ul> <h4 id="deceptive-campaigns-and-misinformation">Deceptive campaigns and misinformation</h4> <ul> <li>Goldstein, J. A., Sastry, G., Musser, M., DiResta, R., Gentzel, M., &amp; Sedova, K. (2023). <a href="http://arxiv.org/abs/2301.04246" rel="external nofollow noopener" target="_blank">Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations</a>.</li> <li>Goldstein, J. A., Chao, J., &amp; Grossman, S. (2024). <a href="https://doi.org/10.1093/pnasnexus/pgae034" rel="external nofollow noopener" target="_blank">How persuasive is AI-generated propaganda?</a> <em>PNAS Nexus</em>, <em>3</em>(2), 1–7.</li> <li>Marcellino, W., Beauchamp-Mustafaga, N., Kerrigan, A., Navarre Chao, L., &amp; Smith, J. (2023). <a href="https://doi.org/10.7249/pea2679-1" rel="external nofollow noopener" target="_blank">The Rise of Generative AI and the Coming Era of Social Media Manipulation 3.0: Next-Generation Chinese Astroturfing and Coping with Ubiquitous AI</a>.</li> <li>Microsoft. (2024). <a href="https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/" rel="external nofollow noopener" target="_blank">Staying ahead of threat actors in the age of AI</a>.</li> <li>Harbarth, K. (2024). <a href="https://anchorchange.substack.com/p/guide-to-the-2024-elections" rel="external nofollow noopener" target="_blank">Guide to the 2024 Elections</a>. <ul> <li>The risk of GenAI for elections may be overestimated. The real risk may be the narrative that GenAI could produce harm</li> </ul> </li> </ul> <h4 id="language-and-its-impact">Language and its impact</h4> <ul> <li>Nicholas, G., &amp; Bhatia, A. (2023). <a href="https://cdt.org/insights/lost-%20in-translation-large-language-models-in-non-english-content-analysis/" rel="external nofollow noopener" target="_blank">Lost in Translation: Large Language Models in Non-English Content Analysis</a>.</li> </ul> <h4 id="ai-incident-trackers">AI incident trackers</h4> <ul> <li><a href="https://oecd.ai/en/incidents" rel="external nofollow noopener" target="_blank">OECD AI Incidents Monitor (AIM)</a></li> </ul> <h4 id="model-transparency">Model transparency</h4> <ul> <li>Bommasani, R., Klyman, K., Kapoor, S., Longpre, S., Xiong, B., Maslej, N., &amp; Liang, P. (2024). <a href="ttps://doi.org/10.48550/arXiv.2407.12929">The Foundation Model Transparency Index v1.1</a>.</li> <li>Bommasani, R., Klyman, K., Longpre, S., Kapoor, S., Maslej, N., Xiong, B., … Liang, P. (2023). <a href="http://arxiv.org/abs/2310.12941" rel="external nofollow noopener" target="_blank">The Foundation Model Transparency Index</a>.</li> </ul> <h4 id="positive-use-cases">Positive use cases</h4> <ul> <li>Argyle, L. P., Busby, E., Gubler, J., Bail, C., Howe, T., Rytting, C., &amp; Wingate, D. (2023). <a href="http://arxiv.org/abs/2302.07268" rel="external nofollow noopener" target="_blank">AI Chat Assistants can Improve Conversations about Divisive Topics</a>.</li> <li>Costello, T. H., Pennycook, G., &amp; Rand, D. G. (2024). <a href="https://doi.org/10.1126/science.adq1814" rel="external nofollow noopener" target="_blank">Durably reducing conspiracy beliefs through dialogues with AI</a>. <em>Science</em>, 385(6714), eadq1814.</li> </ul> <h4 id="perceptions-of-generative-ai">Perceptions of generative AI</h4> <ul> <li>Jakesch, M., Hancock, J. T., &amp; Naaman, M. (2023). <a href="https://doi.org/10.1073/pnas.2208839120" rel="external nofollow noopener" target="_blank">Human heuristics for AI-generated language are flawed</a>. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>(11).</li> <li>Begum Celiktutana, Romain Cadarioa, &amp; Morewedge, C. K. (2017). <a href="https://doi.org/10.1073/pnas" rel="external nofollow noopener" target="_blank">People see more of their biases in algorithms</a>. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>.</li> <li>Altay, S., &amp; Gilardi, F. (2024). <a href="https://doi.org/10.1093/pnasnexus/pgae403" rel="external nofollow noopener" target="_blank">People are skeptical of headlines labeled as AI-generated, even if true or human-made, because they assume full AI automation</a>. <em>PNAS Nexus</em>. <em>3</em>, 1-11.</li> </ul> <h4 id="ai-ethics-classics">AI ethics classics</h4> <ul> <li>Bender, E. M., Gebru, T., Mcmillan-Major, A., &amp; Shmitchell, S. (2021). <a href="https://doi.org/10.1145/3442188.3445922" rel="external nofollow noopener" target="_blank">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a> Conference on Fairness, Accountability, and Transparency (FAccT ‘21), March 310, 2021, Virtual Event, Canada (Vol. 1). Association for Computing Machinery.</li> </ul> <h4 id="ai-policy-overviews">AI policy overviews</h4> <ul> <li>G’sell (2024). <a href="https://cyber.fsi.stanford.edu/content/regulating-under-uncertainty-governance-options-generative-ai" rel="external nofollow noopener" target="_blank">Regulating under Uncertainty: Governance Options for Generative AI</a> <ul> <li>Very comprehensive overview of current AI governance frameworks</li> </ul> </li> <li><a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy" rel="external nofollow noopener" target="_blank">Anthropic’s Responsible Scaling Policy</a></li> <li><a href="https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/" rel="external nofollow noopener" target="_blank">DeepMind’s Frontier Safety Framework</a></li> <li><a href="https://openai.com/preparedness/" rel="external nofollow noopener" target="_blank">OpenAI’s Preparedness Framework</a></li> </ul> <h3 id="ai-safety-related-organizations">AI-safety related organizations</h3> <ul> <li>AI Safety Institutes and Offices <ul> <li><a href="https://www.nist.gov/aisi" rel="external nofollow noopener" target="_blank">US AI Safety Institute</a></li> <li><a href="https://www.aisi.gov.uk/" rel="external nofollow noopener" target="_blank">UK AI Safety Institute</a></li> <li><a href="https://digital-strategy.ec.europa.eu/en/policies/ai-office" rel="external nofollow noopener" target="_blank">EU AI Office</a></li> </ul> </li> <li>The <a href="https://www.frontiermodelforum.org/" rel="external nofollow noopener" target="_blank">Frontier Model Forum</a> is a group of leading AI companies that exchanges on AI safety</li> <li>The <a href="https://cip.org/" rel="external nofollow noopener" target="_blank">Collective Intelligence Project</a> partners with AI companies on gathering democratic input for AI</li> <li> <a href="https://aidatabase.mozilla.org/" rel="external nofollow noopener" target="_blank">Mozilla’s AI Intersections Database</a> lists a range of organizations that work on the societal impact of AI</li> <li> <a href="https://emergingtechpolicy.org/" rel="external nofollow noopener" target="_blank">Emerging Tech Policy Careers</a> lists valuable advice and career resources for those interested in public service careers focused on emerging tech policy</li> <li> <a href="https://metr.org/" rel="external nofollow noopener" target="_blank">METR</a> provides model evaluation and threat research</li> <li> <a href="https://www.dair-institute.org/" rel="external nofollow noopener" target="_blank">Distributed AI Research Institute (DAIR)</a> founded by Timnit Gebru</li> <li>There are also a range of AI consultancies, including <a href="https://www.malosanto.net/" rel="external nofollow noopener" target="_blank">Malo Santo</a>, <a href="https://www.humane-intelligence.org/" rel="external nofollow noopener" target="_blank">Humane Intelligence</a>, <a href="https://www.ethicalintelligence.co/" rel="external nofollow noopener" target="_blank">Ethical Intelligence</a> and <a href="https://www.avanade.com/en-us/about-avanade/partnerships/accenture-avanade-microsoft-alliance" rel="external nofollow noopener" target="_blank">Avanade</a> </li> </ul> <h3 id="mailing-lists">Mailing lists</h3> <ul> <li><a href="https://groups.google.com/g/david-kruegers-80k-people/about" rel="external nofollow noopener" target="_blank">AI Safety Google Group</a></li> <li>Stanford University <ul> <li> <a href="https://mailman.stanford.edu/mailman/listinfo/saic-members" rel="external nofollow noopener" target="_blank">Stanford AI Club</a> for students interested in AI safety</li> <li> <a href="https://mailman.stanford.edu/mailman/listinfo/wonksandtechies2-0" rel="external nofollow noopener" target="_blank">Stanford Wonks &amp; Techies mailing list</a> for students and faculty interested in tech policy</li> </ul> </li> </ul> <h3 id="jobinternship-opportunities">Job/internship opportunities</h3> <ul> <li><a href="https://www.tspa.org/explore/job-board/" rel="external nofollow noopener" target="_blank">Trust and Safety Professional Association Job Board</a></li> <li> <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy" rel="external nofollow noopener" target="_blank">80,000 hours job board</a> (filtered for AI safety &amp; policy)</li> <li><a href="https://stanford.us1.list-manage.com/subscribe?u=a77525a849b0888cf8d90460f&amp;id=807864fbe3" rel="external nofollow noopener" target="_blank">Stanford Public Interest Technology Jobs Newsletter</a></li> <li><a href="https://www.techcongress.io/" rel="external nofollow noopener" target="_blank">Tech Congress Newsletter</a></li> <li> <a href="https://undergradresearch.stanford.edu/fund-your-project/research-fellowships/listing" rel="external nofollow noopener" target="_blank">Stanford Fellowships</a>, including <ul> <li><a href="https://ethicsinsociety.stanford.edu/tech-ethics/career-pathways-professional-development/tech-ethics-policy-rising-scholars-program" rel="external nofollow noopener" target="_blank">Tech Ethics &amp; Policy Rising Scholars Program</a></li> <li><a href="https://fsi.stanford.edu/studentprograms/internships/global-policy-internships" rel="external nofollow noopener" target="_blank">Stanford Global Policy Internships</a></li> <li><a href="https://publicpolicy.stanford.edu/beyond-classroom/internships-career-resources/public-policy-summer-internship-fellowship" rel="external nofollow noopener" target="_blank">Public Policy Summer Internship Fellowship</a></li> <li><a href="https://sig.stanford.edu/internship-opportunities/" rel="external nofollow noopener" target="_blank">Stanford in Government Fellowships and Stipends</a></li> <li><a href="https://sgs.stanford.edu/global-internships/global-studies-internship-program" rel="external nofollow noopener" target="_blank">Global Studies Internship Program</a></li> <li><a href="https://ethicsinsociety.stanford.edu/undergraduate/tech-ethics-policy-summer-fellowships" rel="external nofollow noopener" target="_blank">Tech Ethics &amp; Policy Summer Fellowships</a></li> </ul> </li> </ul> </div> </article> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Ruth E. Appel. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>